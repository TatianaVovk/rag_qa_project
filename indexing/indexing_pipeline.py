import json
import re
from pathlib import Path
from indexing_service.data_loader import load_raw_data, save_cleaned_data
from indexing_service.cleaner import clean_text, is_suspicious



from configs.config import RAW_DATA_PATH, CLEAN_DATA_PATH, MIN_TEXT_LENGTH, DATA_DIR


def log_stats(stats: dict):
    """
    –í—ã–≤–æ–¥–∏—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏.
    """
    print("\n–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ—á–∏—Å—Ç–∫–∏:")
    print(f"  –í—Å–µ–≥–æ –∏—Å—Ö–æ–¥–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤: {stats['total']}")
    print(f"  –ü—É—Å—Ç—ã–µ —Ç–µ–∫—Å—Ç—ã —É–¥–∞–ª–µ–Ω—ã (–¥–æ –æ—á–∏—Å—Ç–∫–∏): {stats['empty']}")
    print(f"  –£–¥–∞–ª–µ–Ω—ã –¥–æ –æ—á–∏—Å—Ç–∫–∏ —Å –±–∏—Ç—ã–º —é–Ω–∏–∫–æ–¥–æ–º: {stats['bad_unicode']}")
    print(f"  –°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Ç–µ–∫—Å—Ç—ã —É–¥–∞–ª–µ–Ω—ã: {stats['too_short']}")
    print(f"  –î—É–±–ª–∏–∫–∞—Ç—ã –ø–æ ID —É–¥–∞–ª–µ–Ω—ã: {stats['duplicate_ids']}")
    print(f"  –î—É–±–ª–∏–∫–∞—Ç—ã –ø–æ —Ç–µ–∫—Å—Ç—É —É–¥–∞–ª–µ–Ω—ã: {stats['duplicate_texts']}")
    print(f"  –û—Å—Ç–∞–ª–æ—Å—å –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏: {stats['valid']}")

def preprocess_text_chunks(chunks: list) -> list:
    """
    –û—á–∏—â–∞–µ—Ç –∏ —Ñ–∏–ª—å—Ç—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –±–ª–æ–∫–∏ (—á–∞–Ω–∫–∏):
    - —É–¥–∞–ª—è–µ—Ç –ø—É—Å—Ç—ã–µ –∏ –∫–æ—Ä–æ—Ç–∫–∏–µ –∑–∞–ø–∏—Å–∏ (<20 —Å–∏–º–≤–æ–ª–æ–≤)
    - —É–¥–∞–ª—è–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ ID –∏ –ø–æ —Ç–µ–∫—Å—Ç—É
    - –ª–æ–≥–∏—Ä—É–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–¥–∞–ª—ë–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ –ø–æ –∫–∞–∂–¥–æ–π –ø—Ä–∏—á–∏–Ω–µ
    - –æ—Å—Ç–∞–≤–ª—è–µ—Ç —Ç–æ–ª—å–∫–æ id –∏ –æ—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
    """
    cleaned = []
    seen_ids = set()
    seen_texts = set()

    stats = {
        "total": len(chunks),
        "too_short": 0,
        "empty": 0,
        "bad_unicode": 0,
        "duplicate_ids": 0,
        "duplicate_texts": 0,
        "valid": 0
    }

    for item in chunks:
        chunk_id = item.get("uid")
        raw_text = item.get("text", "")

        # –Ø–≤–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—É—Å—Ç—ã–µ –∏–ª–∏ "–ø—Å–µ–≤–¥–æ–ø—É—Å—Ç—ã–µ" —Å—Ç—Ä–æ–∫–∏ –¥–æ –æ—á–∏—Å—Ç–∫–∏
        if not raw_text or not raw_text.strip():
            stats["empty"] += 1
            continue

        # üí• –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –±–∏—Ç—ã–µ —Å–∏–º–≤–æ–ª—ã –î–û –æ—á–∏—Å—Ç–∫–∏
        if is_suspicious(raw_text):
            stats["bad_unicode"] += 1
            continue

        # –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
        text = clean_text(raw_text)

        if not text.strip():
            stats["empty"] += 1
            continue

        if len(text) < MIN_TEXT_LENGTH:
            stats["too_short"] += 1
            continue

        if chunk_id in seen_ids:
            stats["duplicate_ids"] += 1
            continue

        if text in seen_texts:
            stats["duplicate_texts"] += 1
            continue

        cleaned.append({"id": chunk_id, "text": text})
        seen_ids.add(chunk_id)
        seen_texts.add(text)
        stats["valid"] += 1

    log_stats(stats)
    return cleaned

if __name__ == "__main__":
    # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ –ø–∞–ø–∫–∞ data —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
    Path(DATA_DIR).mkdir(exist_ok=True)

    raw = load_raw_data(RAW_DATA_PATH)
    cleaned = preprocess_text_chunks(raw)
    save_cleaned_data(cleaned, CLEAN_DATA_PATH)

    print("\n–û—á–∏—Å—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞.")












